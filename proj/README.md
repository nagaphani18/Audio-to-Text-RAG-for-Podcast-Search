# Audio-to-Text RAG for Podcast Search

A multimodal RAG (Retrieval-Augmented Generation) system that processes audio podcasts, converts them to searchable text, and allows users to query specific topics mentioned across multiple episodes with timestamp references.

## ğŸ¯ Features

- **High-accuracy audio-to-text conversion** using Whisper
- **Multi-episode search** across podcast libraries
- **Topic-based querying** with contextual understanding
- **Timestamp referencing** for precise audio segment location
- **Speaker identification and diarization** for multi-speaker podcasts
- **Vector-based semantic search** using sentence transformers
- **Clean web interface** with Streamlit

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Audio Input   â”‚â”€â”€â”€â–¶â”‚  Whisper STT    â”‚â”€â”€â”€â–¶â”‚  Text Chunks    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Query    â”‚â—€â”€â”€â”€â”‚  Vector Search  â”‚â—€â”€â”€â”€â”‚  Embeddings     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Response   â”‚    â”‚  Timestamp Ref  â”‚    â”‚  Chroma DB      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- FFmpeg (for audio processing)
- OpenAI API key (optional, for enhanced responses)

### Installation

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd audio-to-text-rag
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

4. **Run the application**
   ```bash
   streamlit run app.py
   ```

## ğŸ“ Project Structure

```
audio-to-text-rag/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ audio_processor.py # Audio preprocessing and STT
â”‚   â”œâ”€â”€ text_processor.py  # Text chunking and processing
â”‚   â”œâ”€â”€ embeddings.py      # Vector embeddings and search
â”‚   â”œâ”€â”€ rag_engine.py      # RAG pipeline orchestration
â”‚   â””â”€â”€ utils.py           # Utility functions
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ audio/             # Audio files storage
â”‚   â”œâ”€â”€ transcripts/       # Generated transcripts
â”‚   â””â”€â”€ database/          # Vector database
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ evaluation.ipynb   # System evaluation
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_*.py          # Unit tests
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env.example          # Environment variables template
â””â”€â”€ README.md             # This file
```

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file with the following variables:

```env
# OpenAI API (optional, for enhanced responses)
OPENAI_API_KEY=your_openai_api_key

# HuggingFace (for embeddings)
HUGGINGFACE_API_KEY=your_huggingface_token

# Model configurations
WHISPER_MODEL=base  # Options: tiny, base, small, medium, large
EMBEDDING_MODEL=all-MiniLM-L6-v2
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
```

### Model Options

- **Whisper Models**: `tiny`, `base`, `small`, `medium`, `large`
- **Embedding Models**: `all-MiniLM-L6-v2`, `all-mpnet-base-v2`, `multi-qa-MiniLM-L6-v2`

## ğŸ“Š Usage

### 1. Upload Audio Files

- Supported formats: MP3, WAV, M4A, FLAC
- Maximum file size: 100MB per file
- Batch upload supported

### 2. Process Episodes

- Automatic audio-to-text conversion
- Speaker diarization (if multiple speakers detected)
- Timestamp generation for each segment
- Vector embedding creation

### 3. Search and Query

- Natural language queries
- Topic-based search across all episodes
- Timestamp references for audio segments
- Relevance scoring

### 4. Example Queries

- "What did they say about machine learning?"
- "Find discussions about climate change"
- "Show me segments about startup funding"
- "What are the main points about AI ethics?"

## ğŸ§ª Evaluation

The system includes evaluation metrics:

- **Retrieval Accuracy**: Precision@K, Recall@K
- **Response Relevance**: RAGAS metrics
- **Latency**: Query response time
- **Audio Quality**: Transcription accuracy

Run evaluation:
```bash
python -m pytest tests/
jupyter notebook notebooks/evaluation.ipynb
```

## ğŸ” Technical Details

### Audio Processing Pipeline

1. **Preprocessing**: Noise reduction, normalization
2. **STT Conversion**: OpenAI Whisper for transcription
3. **Speaker Diarization**: Pyannote.audio for speaker identification
4. **Timestamp Mapping**: Precise time alignment

### Text Processing

1. **Chunking**: Semantic-aware text segmentation
2. **Embedding**: Sentence transformers for vector representation
3. **Indexing**: Chroma vector database for fast retrieval

### RAG Pipeline

1. **Query Processing**: Embedding generation
2. **Retrieval**: Vector similarity search
3. **Reranking**: Relevance scoring
4. **Generation**: Context-aware response generation

## ğŸ› ï¸ Development

### Adding New Features

1. **Custom Embeddings**: Modify `src/embeddings.py`
2. **Audio Processing**: Extend `src/audio_processor.py`
3. **UI Components**: Update `app.py`

### Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src

# Run specific test
pytest tests/test_audio_processor.py
```

## ğŸ“ˆ Performance Optimization

- **Batch Processing**: Parallel audio processing
- **Caching**: Embedding and transcription caching
- **Indexing**: Efficient vector database queries
- **Streaming**: Real-time audio processing

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- OpenAI for Whisper
- HuggingFace for sentence transformers
- Chroma for vector database
- Streamlit for the web interface

## ğŸ“ Support

For issues and questions:
- Create an issue on GitHub
- Check the documentation
- Review the evaluation notebook

---

**Demo Links:**
- [Live Application](https://your-app-url.streamlit.app)
- [GitHub Repository](https://github.com/your-username/audio-to-text-rag) 